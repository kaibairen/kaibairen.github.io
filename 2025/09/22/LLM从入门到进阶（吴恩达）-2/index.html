<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>LLM从入门到进阶（吴恩达）- 2 | 开摆人のBlog</title><meta name="author" content="学业开摆人"><meta name="copyright" content="学业开摆人"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="参数高效微调（PEFT）相较于全量微调，参数高效微调只更新小部分参数，大体方式如下：  冻结了大部分的模型权重，专注于微调现有的模型参数子集（特定的层或组件） 不触碰原始模型的权重，而是添加少量的参数或层，只微调新的组件  PEFT Method Selective Method（选择性方法）：只微调原始参数子集 选择特定组件或者特定层 选择单个参数类型   Reparameterization">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM从入门到进阶（吴恩达）- 2">
<meta property="og:url" content="http://kaibairen.github.io/2025/09/22/LLM%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E8%BF%9B%E9%98%B6%EF%BC%88%E5%90%B4%E6%81%A9%E8%BE%BE%EF%BC%89-2/index.html">
<meta property="og:site_name" content="开摆人のBlog">
<meta property="og:description" content="参数高效微调（PEFT）相较于全量微调，参数高效微调只更新小部分参数，大体方式如下：  冻结了大部分的模型权重，专注于微调现有的模型参数子集（特定的层或组件） 不触碰原始模型的权重，而是添加少量的参数或层，只微调新的组件  PEFT Method Selective Method（选择性方法）：只微调原始参数子集 选择特定组件或者特定层 选择单个参数类型   Reparameterization">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://files.codelife.cc/wallhaven/full/dg/wallhaven-dg8p1m.jpg?x-oss-process=image/resize,limit_0,m_fill,w_2560,h_1440/quality,Q_93/format,webp">
<meta property="article:published_time" content="2025-09-22T04:00:47.000Z">
<meta property="article:modified_time" content="2025-09-22T07:38:55.544Z">
<meta property="article:author" content="学业开摆人">
<meta property="article:tag" content="学习笔记">
<meta property="article:tag" content="LLM课程">
<meta property="article:tag" content="吴恩达">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://files.codelife.cc/wallhaven/full/dg/wallhaven-dg8p1m.jpg?x-oss-process=image/resize,limit_0,m_fill,w_2560,h_1440/quality,Q_93/format,webp"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "LLM从入门到进阶（吴恩达）- 2",
  "url": "http://kaibairen.github.io/2025/09/22/LLM%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E8%BF%9B%E9%98%B6%EF%BC%88%E5%90%B4%E6%81%A9%E8%BE%BE%EF%BC%89-2/",
  "image": "https://files.codelife.cc/wallhaven/full/dg/wallhaven-dg8p1m.jpg?x-oss-process=image/resize,limit_0,m_fill,w_2560,h_1440/quality,Q_93/format,webp",
  "datePublished": "2025-09-22T04:00:47.000Z",
  "dateModified": "2025-09-22T07:38:55.544Z",
  "author": [
    {
      "@type": "Person",
      "name": "学业开摆人",
      "url": "http://kaibairen.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://kaibairen.github.io/2025/09/22/LLM%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E8%BF%9B%E9%98%B6%EF%BC%88%E5%90%B4%E6%81%A9%E8%BE%BE%EF%BC%89-2/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LLM从入门到进阶（吴恩达）- 2',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="https://s21.ax1x.com/2025/08/11/pVd6BWD.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">8</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">15</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://files.codelife.cc/wallhaven/full/dg/wallhaven-dg8p1m.jpg?x-oss-process=image/resize,limit_0,m_fill,w_2560,h_1440/quality,Q_93/format,webp);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">开摆人のBlog</span></a><a class="nav-page-title" href="/"><span class="site-name">LLM从入门到进阶（吴恩达）- 2</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">LLM从入门到进阶（吴恩达）- 2</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-09-22T04:00:47.000Z" title="发表于 2025-09-22 12:00:47">2025-09-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-22T07:38:55.544Z" title="更新于 2025-09-22 15:38:55">2025-09-22</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/LLM%E8%AF%BE%E7%A8%8B/">LLM课程</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/LLM%E8%AF%BE%E7%A8%8B/%E5%90%B4%E6%81%A9%E8%BE%BE/">吴恩达</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="参数高效微调（PEFT）"><a href="#参数高效微调（PEFT）" class="headerlink" title="参数高效微调（PEFT）"></a>参数高效微调（PEFT）</h1><p>相较于全量微调，参数高效微调只更新小部分参数，大体方式如下：</p>
<ol>
<li>冻结了大部分的模型权重，专注于微调现有的模型参数子集（特定的层或组件）</li>
<li>不触碰原始模型的权重，而是添加少量的参数或层，只微调新的组件</li>
</ol>
<h2 id="PEFT-Method"><a href="#PEFT-Method" class="headerlink" title="PEFT Method"></a>PEFT Method</h2><ol>
<li>Selective Method（选择性方法）：只微调原始参数子集<ul>
<li>选择特定组件或者特定层</li>
<li>选择单个参数类型</li>
</ul>
</li>
<li>Reparameterization Method（重新参数化方法）：通过创建原始网络权重的新低秩矩阵转换减少要训练的参数数量（如LoRA）</li>
<li>Additive Method（加法方法）：通过保留所有原始LLM权重进行微调并引入新的可训练组件，如下：<ul>
<li>Adapters：在模型架构中添加新的可训练层，通常在编码器（Encoder）或者解码器（Decoder）的注意力或前馈层之后</li>
<li>Soft Prompt：保持模型解构的固定和冻结，并专注于操纵输入以获得更好的性能。这可以通过向提示词嵌入（Prompt Embeddings）添加可训练参数或保持输入固定并重新训练嵌入权重来完成–&gt;<code>Prompt Tuning</code></li>
</ul>
</li>
</ol>
<blockquote>
<p>每种方法在参数效率，内存效率，训练速度，模型质量和推理成本上都有权衡</p>
</blockquote>
<h1 id="Lora（Low-Rank-Adaptation）"><a href="#Lora（Low-Rank-Adaptation）" class="headerlink" title="Lora（Low-Rank Adaptation）"></a>Lora（Low-Rank Adaptation）</h1><p>Lora通过冻结原始模型的所有参数，并在某些特定层注入两个低秩矩阵 A 和 B，这些低秩矩阵包含了可训练的参数，通过只训练极少的参数，达到全量微调相近的性能：<br><img src="./Lora.png" alt="加载失败" title="Lora_intro"></p>
<table>
<thead>
<tr>
<th align="center">符号</th>
<th align="center">变量名</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewbox="0 -442 572 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g></g></g></svg></mjx-container></td>
<td align="center">输入向量</td>
</tr>
<tr>
<td align="center"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex;" xmlns="http://www.w3.org/2000/svg" width="3.123ex" height="1.92ex" role="img" focusable="false" viewbox="0 -683 1380.6 848.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"/></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"/></g></g></g></g></svg></mjx-container></td>
<td align="center">预训练权重</td>
</tr>
<tr>
<td align="center"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="4.382ex" height="1.67ex" role="img" focusable="false" viewbox="0 -716 1937 738"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="25B3" d="M75 0L72 2Q69 3 67 5T62 11T59 20Q59 24 62 30Q65 37 245 370T428 707Q428 708 430 710T436 714T444 716Q451 716 455 712Q459 710 644 368L828 27V20Q828 7 814 0H75ZM610 347L444 653Q443 653 278 347T113 40H775Q775 42 610 347Z"/></g><g data-mml-node="mi" transform="translate(889,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"/></g></g></g></svg></mjx-container></td>
<td align="center">增量参数矩阵</td>
</tr>
<tr>
<td align="center"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="5.532ex" height="1.645ex" role="img" focusable="false" viewbox="0 -716 2445 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"/></g><g data-mml-node="mi" transform="translate(759,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"/></g><g data-mml-node="mi" transform="translate(1244,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mi" transform="translate(1695,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"/></g></g></g></svg></mjx-container></td>
<td align="center">低秩适应的权重矩阵(r &lt;&lt; d)</td>
</tr>
</tbody></table>
<p>可以发现，微调的参数量从原先的<code>d*d</code>, 变成了<code>2*d*r</code>,大大减少的参数量，从而显著减少显存占用。对于LoRA微调，在训练时，会冻结预训练的权重<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex;" xmlns="http://www.w3.org/2000/svg" width="3.123ex" height="1.92ex" role="img" focusable="false" viewbox="0 -683 1380.6 848.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"/></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"/></g></g></g></g></svg></mjx-container>，这意味着在前向传播和反向传播过程中，不会计算其对应的梯度 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.047ex;" xmlns="http://www.w3.org/2000/svg" width="4.11ex" height="3.082ex" role="img" focusable="false" viewbox="0 -899.6 1816.4 1362.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mrow" transform="translate(467.3,394) scale(0.707)"><g data-mml-node="mi"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"/></g><g data-mml-node="mi" transform="translate(566,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"/></g></g><g data-mml-node="mrow" transform="translate(220,-345.6) scale(0.707)"><g data-mml-node="mi"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"/></g><g data-mml-node="msub" transform="translate(566,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"/></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"/></g></g></g><rect width="1576.4" height="60" x="120" y="220"/></g></g></g></svg></mjx-container></p>
<ul>
<li><h2 id="step-1-初始化"><a href="#step-1-初始化" class="headerlink" title="step 1: 初始化"></a>step 1: 初始化</h2></li>
</ul>
<p>在初始化时，令 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="14.211ex" height="1.805ex" role="img" focusable="false" viewbox="0 -716 6281.1 798"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="25B3" d="M75 0L72 2Q69 3 67 5T62 11T59 20Q59 24 62 30Q65 37 245 370T428 707Q428 708 430 710T436 714T444 716Q451 716 455 712Q459 710 644 368L828 27V20Q828 7 814 0H75ZM610 347L444 653Q443 653 278 347T113 40H775Q775 42 610 347Z"/></g><g data-mml-node="mi" transform="translate(889,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="mo" transform="translate(1882.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mi" transform="translate(2938.6,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"/></g><g data-mml-node="mi" transform="translate(3697.6,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"/></g><g data-mml-node="mo" transform="translate(4725.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(5781.1,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"/></g></g></g></svg></mjx-container></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://kaibairen.github.io">学业开摆人</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://kaibairen.github.io/2025/09/22/LLM%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E8%BF%9B%E9%98%B6%EF%BC%88%E5%90%B4%E6%81%A9%E8%BE%BE%EF%BC%89-2/">http://kaibairen.github.io/2025/09/22/LLM%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E8%BF%9B%E9%98%B6%EF%BC%88%E5%90%B4%E6%81%A9%E8%BE%BE%EF%BC%89-2/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://kaibairen.github.io" target="_blank">开摆人のBlog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><a class="post-meta__tags" href="/tags/LLM%E8%AF%BE%E7%A8%8B/">LLM课程</a><a class="post-meta__tags" href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE/">吴恩达</a></div><div class="post-share"><div class="social-share" data-image="https://files.codelife.cc/wallhaven/full/dg/wallhaven-dg8p1m.jpg?x-oss-process=image/resize,limit_0,m_fill,w_2560,h_1440/quality,Q_93/format,webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/11/25/game-1/" title="LLM基座CoT推理性能优化研究工作说明"><img class="cover" src="https://files.codelife.cc/wallhaven/full/5w/wallhaven-5we787.jpg?x-oss-process=image/resize,limit_0,m_fill,w_2560,h_1440/quality,Q_93/format,webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">LLM基座CoT推理性能优化研究工作说明</div></div><div class="info-2"><div class="info-item-1">研究背景与核心现象发现1.1 研究切入点：思维链方法的应用与观察&emsp;&emsp;本研究以思维链（Chain of Thought, CoT）及表格化思维链（Tab-CoT）等CoT变式的prompt工程方法为核心工具，针对大语言模型（LLM）基座在不同类型的benchmark中的推理性能展开系统性探究。旨在系统性探究大语言模型基座在不同类型测试基准中的推理性能边界。通过在多类任务场景下部署标准化思维链提示，对比分析大语言模型在不同领域的表现差异，从而识别当前模型在特定任务类型上的性能瓶颈，并以此为基础探索优化模型综合效能的可行路径。 1.2  核心现象：LLM基座在选择题基准上的性能短板&emsp;&emsp;基于思维链与表格化思维链方法的广泛测试与深入分析，本研究确立了一项关键实证发现：大语言模型基座在以AQUA为代表的数学推理类选择题基准上，呈现出显著的性能短板。相较于在非选择题或通用知识类基准上的表现，模型在处理此类需要严密逻辑推导的结构化选择题时，准确率与推理稳定性均存在明显下滑。具体性能差异趋势如图1所示。 研究数据设计与准备2.1 基准数据集选用原则&emsp...</div></div></div></a><a class="pagination-related" href="/2025/09/15/LLM%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E8%BF%9B%E9%98%B6%EF%BC%88%E5%90%B4%E6%81%A9%E8%BE%BE%EF%BC%89/" title="LLM从入门到进阶（吴恩达）- 1"><img class="cover" src="https://files.codelife.cc/wallhaven/full/dg/wallhaven-dg8p1m.jpg?x-oss-process=image/resize,limit_0,m_fill,w_2560,h_1440/quality,Q_93/format,webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">LLM从入门到进阶（吴恩达）- 1</div></div><div class="info-2"><div class="info-item-1">1.1 对单一任务进行微调灾难性遗忘（Catastrophic Forgetting）现象简述原因： 全量微调过程中改变LLM原始的模型权重表现：尽管可能在微调目标的单一任务上表现良好，但是会降低在其他任务上的性能 解决方法 确定灾难性遗忘是否真的会影响到你的使用场景 在只针对特定任务场景的情况下，模型的泛化能力并不被需要，则不会造成什么影响   同时在多个任务下进行微调 执行参数有效微调 Parameter Efficient Fine-tuning（PEFT） PEFT保留了模型的原始权重，并且只训练少量特定任务的适配器层（adapter layers）和参数    1.2 Evaluation MetricROUGE（召回率导向的摘要评估）ROUGE主要用于通过将自动生成的摘要和人工生成的参考摘要进行比较来评估生成文本的质量  Used for text summarizationcompares a summary to one or more reference summaries  在了解ROUGE的公式前，我们需要了解语言解剖学中的相关信息gram。如上图所示，在句...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/09/15/LLM%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E8%BF%9B%E9%98%B6%EF%BC%88%E5%90%B4%E6%81%A9%E8%BE%BE%EF%BC%89/" title="LLM从入门到进阶（吴恩达）- 1"><img class="cover" src="https://files.codelife.cc/wallhaven/full/dg/wallhaven-dg8p1m.jpg?x-oss-process=image/resize,limit_0,m_fill,w_2560,h_1440/quality,Q_93/format,webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-15</div><div class="info-item-2">LLM从入门到进阶（吴恩达）- 1</div></div><div class="info-2"><div class="info-item-1">1.1 对单一任务进行微调灾难性遗忘（Catastrophic Forgetting）现象简述原因： 全量微调过程中改变LLM原始的模型权重表现：尽管可能在微调目标的单一任务上表现良好，但是会降低在其他任务上的性能 解决方法 确定灾难性遗忘是否真的会影响到你的使用场景 在只针对特定任务场景的情况下，模型的泛化能力并不被需要，则不会造成什么影响   同时在多个任务下进行微调 执行参数有效微调 Parameter Efficient Fine-tuning（PEFT） PEFT保留了模型的原始权重，并且只训练少量特定任务的适配器层（adapter layers）和参数    1.2 Evaluation MetricROUGE（召回率导向的摘要评估）ROUGE主要用于通过将自动生成的摘要和人工生成的参考摘要进行比较来评估生成文本的质量  Used for text summarizationcompares a summary to one or more reference summaries  在了解ROUGE的公式前，我们需要了解语言解剖学中的相关信息gram。如上图所示，在句...</div></div></div></a><a class="pagination-related" href="/2025/09/05/%E7%AE%97%E6%B3%95%E5%9B%BE%E8%A7%A3reading-1/" title="算法图解学习笔记-1"><img class="cover" src="https://files.codelife.cc/wallhaven/full/dg/wallhaven-dg8p1m.jpg?x-oss-process=image/resize,limit_0,m_fill,w_2560,h_1440/quality,Q_93/format,webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-05</div><div class="info-item-2">算法图解学习笔记-1</div></div><div class="info-2"><div class="info-item-1">二分查找1.1 示例code12345678910111213def erfen_search(arr, target):    low = 0    high = len(arr)    while low &lt;= high:        mid = (high - low)//2        guesss = arr[mid]        if guess &lt; target:            high = mid - 1        elif guess == target:            return mid        else:            low = mid + 1        return None  数组和链表需要存储多项数据时，存在两种基本方式——数组和链表。  需要明确的是数组和链表并非都适用于所有的情形，因此我们需要知道他们的差别&amp;优缺点。  数组使用数组意味着所有的元素在内存中都是相连的（需要分配连续的内存空间）。  在数组中添加新元素时，若当前连续空间后的内存已被占用，则需要重新分配连续的内存。  我...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="https://s21.ax1x.com/2025/08/11/pVd6BWD.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">学业开摆人</div><div class="author-info-description">大叔我啊要去拯救世界了~</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">8</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">15</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/kaibairen"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Welcome to kaikairen's blog!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83%EF%BC%88PEFT%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">参数高效微调（PEFT）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#PEFT-Method"><span class="toc-number">1.1.</span> <span class="toc-text">PEFT Method</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Lora%EF%BC%88Low-Rank-Adaptation%EF%BC%89"><span class="toc-number">2.</span> <span class="toc-text">Lora（Low-Rank Adaptation）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#step-1-%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">2.1.</span> <span class="toc-text">step 1: 初始化</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/11/25/game-1/" title="LLM基座CoT推理性能优化研究工作说明"><img src="https://files.codelife.cc/wallhaven/full/5w/wallhaven-5we787.jpg?x-oss-process=image/resize,limit_0,m_fill,w_2560,h_1440/quality,Q_93/format,webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LLM基座CoT推理性能优化研究工作说明"/></a><div class="content"><a class="title" href="/2025/11/25/game-1/" title="LLM基座CoT推理性能优化研究工作说明">LLM基座CoT推理性能优化研究工作说明</a><time datetime="2025-11-25T05:40:55.000Z" title="发表于 2025-11-25 13:40:55">2025-11-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/09/22/LLM%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E8%BF%9B%E9%98%B6%EF%BC%88%E5%90%B4%E6%81%A9%E8%BE%BE%EF%BC%89-2/" title="LLM从入门到进阶（吴恩达）- 2"><img src="https://files.codelife.cc/wallhaven/full/dg/wallhaven-dg8p1m.jpg?x-oss-process=image/resize,limit_0,m_fill,w_2560,h_1440/quality,Q_93/format,webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LLM从入门到进阶（吴恩达）- 2"/></a><div class="content"><a class="title" href="/2025/09/22/LLM%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E8%BF%9B%E9%98%B6%EF%BC%88%E5%90%B4%E6%81%A9%E8%BE%BE%EF%BC%89-2/" title="LLM从入门到进阶（吴恩达）- 2">LLM从入门到进阶（吴恩达）- 2</a><time datetime="2025-09-22T04:00:47.000Z" title="发表于 2025-09-22 12:00:47">2025-09-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/09/15/LLM%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E8%BF%9B%E9%98%B6%EF%BC%88%E5%90%B4%E6%81%A9%E8%BE%BE%EF%BC%89/" title="LLM从入门到进阶（吴恩达）- 1"><img src="https://files.codelife.cc/wallhaven/full/dg/wallhaven-dg8p1m.jpg?x-oss-process=image/resize,limit_0,m_fill,w_2560,h_1440/quality,Q_93/format,webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LLM从入门到进阶（吴恩达）- 1"/></a><div class="content"><a class="title" href="/2025/09/15/LLM%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E8%BF%9B%E9%98%B6%EF%BC%88%E5%90%B4%E6%81%A9%E8%BE%BE%EF%BC%89/" title="LLM从入门到进阶（吴恩达）- 1">LLM从入门到进阶（吴恩达）- 1</a><time datetime="2025-09-15T01:14:32.000Z" title="发表于 2025-09-15 09:14:32">2025-09-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/09/05/%E7%AE%97%E6%B3%95%E5%9B%BE%E8%A7%A3reading-1/" title="算法图解学习笔记-1"><img src="https://files.codelife.cc/wallhaven/full/dg/wallhaven-dg8p1m.jpg?x-oss-process=image/resize,limit_0,m_fill,w_2560,h_1440/quality,Q_93/format,webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="算法图解学习笔记-1"/></a><div class="content"><a class="title" href="/2025/09/05/%E7%AE%97%E6%B3%95%E5%9B%BE%E8%A7%A3reading-1/" title="算法图解学习笔记-1">算法图解学习笔记-1</a><time datetime="2025-09-05T07:12:35.000Z" title="发表于 2025-09-05 15:12:35">2025-09-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/09/02/llama-factory/" title="在华为云部署GLM-4-9B(基于LLaMA-Factory)"><img src="https://files.codelife.cc/wallhaven/full/dg/wallhaven-dg8p1m.jpg?x-oss-process=image/resize,limit_0,m_fill,w_2560,h_1440/quality,Q_93/format,webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="在华为云部署GLM-4-9B(基于LLaMA-Factory)"/></a><div class="content"><a class="title" href="/2025/09/02/llama-factory/" title="在华为云部署GLM-4-9B(基于LLaMA-Factory)">在华为云部署GLM-4-9B(基于LLaMA-Factory)</a><time datetime="2025-09-02T06:26:04.000Z" title="发表于 2025-09-02 14:26:04">2025-09-02</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(https://files.codelife.cc/wallhaven/full/jx/wallhaven-jxl3qp.jpg?x-oss-process=image/resize,limit_0,m_fill,w_1920,h_1080/quality,Q_93/format,webp);"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By 学业开摆人</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.3</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>